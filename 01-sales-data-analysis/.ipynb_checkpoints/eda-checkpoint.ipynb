{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "# Sales Data Analysis\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This project uses [Knightbearr's sales product data](https://www.kaggle.com/datasets/knightbearr/sales-product-data) which, though contrived, serves as a useful starting point for store sales data analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {
    "pycharm": {
     "is_executing": true,
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "import pandas as pd"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Getting the data"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I downloaded the data archive, which comes with twelve files, each containing data for one of the twelve months of the year. To load the data into a Pandas DataFrame, I used the file name convention to my advantage:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "months = [\n",
    "    \"January\",\n",
    "    \"February\",\n",
    "    \"March\",\n",
    "    \"April\",\n",
    "    \"May\",\n",
    "    \"June\",\n",
    "    \"July\",\n",
    "    \"August\",\n",
    "    \"September\",\n",
    "    \"October\",\n",
    "    \"November\",\n",
    "    \"December\",\n",
    "]\n",
    "\n",
    "df = pd.DataFrame()\n",
    "\n",
    "for month in months:\n",
    "    df = pd.concat([df, pd.read_csv(\"./data/knightbearr-sales-data/Sales_{0}_2019.csv\".format(month))], ignore_index = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Note that `ignore_index` is set to `True` in the `concat()` call; this is because each DataFrame loaded by the `read_csv()` call in the loop has its own indices and if we don't ignore them, the repetition causes problems with DataFrame concatenation.\n",
    "\n",
    "As an experiemnt, try to replicate this whole notebook without using the `ignore_index` parameter. You'll see what issues I mean when you get to the grouped monthly sales data.\n",
    "\n",
    "For now, let's see what sort of data we have:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 186850 entries, 0 to 186849\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Order ID          186305 non-null  object\n",
      " 1   Product           186305 non-null  object\n",
      " 2   Quantity Ordered  186305 non-null  object\n",
      " 3   Price Each        186305 non-null  object\n",
      " 4   Order Date        186305 non-null  object\n",
      " 5   Purchase Address  186305 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 8.6+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `object`s in the `Dtype` column indicate that all the columns contain strings, which means we'll have to parse the strings in the `Order ID`, `Quantity Ordered`, `Price Each`, and `Order Date` colummns to get numeric values.\n",
    "\n",
    "First, though, let's see check the data quality."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Checking data quality"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Missing values"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The simplest data qualit check is seeing if there are missing values. Pandas DataFrames have a convenient `innull()` method that returns a DataFrame with the same dimensions as the DataFrame on which it was called, but filled with booleans indicating whether the value at that row and column is null:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            545\n",
       "Product             545\n",
       "Quantity Ordered    545\n",
       "Price Each          545\n",
       "Order Date          545\n",
       "Purchase Address    545\n",
       "dtype: int64"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Interesing fact: the pandas DataFrame methods `isnull()` and `isna()` are actually the exact same method, according to [this](https://datascience.stackexchange.com/questions/37878/difference-between-isna-and-isnull-in-pandas) Data Science StackExchange answer. The functionality was given two different names for the benefit of R users, because in R a DataFrame can have null as well as NA values, but Python has neither and instead has NaN values!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like 545 entries in each column are null-valued. My bet is that there are 545 rows with all null values, rather than 545 entries in each column being randomly null-valued. The latter is too much coincidence: if there _were_ random nulls in each column, what are the odds the number of nulls in every column would be the same?\n",
    "\n",
    "Just to satisfy curiosity, let's get the rows with null values and see what we can make of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>664</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>678</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>797</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>876</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1299</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184678</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>184695</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185042</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>185729</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186463</th>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "      <td>NaN</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>545 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID Product Quantity Ordered Price Each Order Date  \\\n",
       "664         NaN     NaN              NaN        NaN        NaN   \n",
       "678         NaN     NaN              NaN        NaN        NaN   \n",
       "797         NaN     NaN              NaN        NaN        NaN   \n",
       "876         NaN     NaN              NaN        NaN        NaN   \n",
       "1299        NaN     NaN              NaN        NaN        NaN   \n",
       "...         ...     ...              ...        ...        ...   \n",
       "184678      NaN     NaN              NaN        NaN        NaN   \n",
       "184695      NaN     NaN              NaN        NaN        NaN   \n",
       "185042      NaN     NaN              NaN        NaN        NaN   \n",
       "185729      NaN     NaN              NaN        NaN        NaN   \n",
       "186463      NaN     NaN              NaN        NaN        NaN   \n",
       "\n",
       "       Purchase Address  \n",
       "664                 NaN  \n",
       "678                 NaN  \n",
       "797                 NaN  \n",
       "876                 NaN  \n",
       "1299                NaN  \n",
       "...                 ...  \n",
       "184678              NaN  \n",
       "184695              NaN  \n",
       "185042              NaN  \n",
       "185729              NaN  \n",
       "186463              NaN  \n",
       "\n",
       "[545 rows x 6 columns]"
      ]
     },
     "execution_count": 5,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df.isnull().any(axis=1)]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As expected, there are 545 rows filled with NaNs. What's less expected is that they appear to be randomly distributed, based on a quick glance at the indices. I suppose I expected that they would occur at somewhat regular intervals, e.g. at the end of each of the monthly data files.\n",
    "\n",
    "Since we know that all the NaN-valued rows are completely useless, we can drop them:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.dropna(inplace=True)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 7,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.isnull().sum()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. Now let's perform some other data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data type consistency"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>51493</th>\n",
       "      <td>190392</td>\n",
       "      <td>Flatscreen TV</td>\n",
       "      <td>1</td>\n",
       "      <td>300</td>\n",
       "      <td>04/09/19 19:00</td>\n",
       "      <td>466 Hill St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>23088</th>\n",
       "      <td>163266</td>\n",
       "      <td>27in 4K Gaming Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>389.99</td>\n",
       "      <td>03/24/19 20:18</td>\n",
       "      <td>951 Lincoln St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141023</th>\n",
       "      <td>275885</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>10/05/19 07:15</td>\n",
       "      <td>904 Cherry St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>6251</th>\n",
       "      <td>147181</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>01/02/19 14:40</td>\n",
       "      <td>480 Forest St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>46649</th>\n",
       "      <td>185782</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>2</td>\n",
       "      <td>2.99</td>\n",
       "      <td>04/20/19 16:29</td>\n",
       "      <td>116 Madison St, Atlanta, GA 30301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>61972</th>\n",
       "      <td>200364</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>05/16/19 18:24</td>\n",
       "      <td>993 Spruce St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>140374</th>\n",
       "      <td>275267</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>10/05/19 21:02</td>\n",
       "      <td>697 2nd St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>124723</th>\n",
       "      <td>260335</td>\n",
       "      <td>34in Ultrawide Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>379.99</td>\n",
       "      <td>10/09/19 18:49</td>\n",
       "      <td>141 Jackson St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>72832</th>\n",
       "      <td>210712</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>06/24/19 05:00</td>\n",
       "      <td>392 Lake St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>110428</th>\n",
       "      <td>246653</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>08/10/19 12:58</td>\n",
       "      <td>633 Lakeview St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                     Product Quantity Ordered Price Each  \\\n",
       "51493    190392               Flatscreen TV                1        300   \n",
       "23088    163266      27in 4K Gaming Monitor                1     389.99   \n",
       "141023   275885    Lightning Charging Cable                1      14.95   \n",
       "6251     147181        USB-C Charging Cable                1      11.95   \n",
       "46649    185782      AAA Batteries (4-pack)                2       2.99   \n",
       "61972    200364            Wired Headphones                1      11.99   \n",
       "140374   275267  Bose SoundSport Headphones                1      99.99   \n",
       "124723   260335      34in Ultrawide Monitor                1     379.99   \n",
       "72832    210712        USB-C Charging Cable                1      11.95   \n",
       "110428   246653                      iPhone                1        700   \n",
       "\n",
       "            Order Date                          Purchase Address  \n",
       "51493   04/09/19 19:00      466 Hill St, San Francisco, CA 94016  \n",
       "23088   03/24/19 20:18         951 Lincoln St, Seattle, WA 98101  \n",
       "141023  10/05/19 07:15    904 Cherry St, San Francisco, CA 94016  \n",
       "6251    01/02/19 14:40           480 Forest St, Dallas, TX 75001  \n",
       "46649   04/20/19 16:29         116 Madison St, Atlanta, GA 30301  \n",
       "61972   05/16/19 18:24      993 Spruce St, Los Angeles, CA 90001  \n",
       "140374  10/05/19 21:02       697 2nd St, San Francisco, CA 94016  \n",
       "124723  10/09/19 18:49         141 Jackson St, Seattle, WA 98101  \n",
       "72832   06/24/19 05:00             392 Lake St, Austin, TX 73301  \n",
       "110428  08/10/19 12:58  633 Lakeview St, San Francisco, CA 94016  "
      ]
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That gives us an idea of what formats each column should be converted to.\n",
    "\n",
    "Looks like `Order ID` should have one of Python's integer data types. Let's see if any entries don't:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            355\n",
       "Product             355\n",
       "Quantity Ordered    355\n",
       "Price Each          355\n",
       "Order Date          355\n",
       "Purchase Address    355\n",
       "dtype: int64"
      ]
     },
     "execution_count": 9,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Order ID\"].str.isdecimal() == False].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Fascinating! Looks like 355 rows contain non-numeric values in every column. Let's check them out."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>121332</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>58411</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>34529</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>169827</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>147389</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>88576</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2463</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>123475</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>166876</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>141764</th>\n",
       "      <td>Order ID</td>\n",
       "      <td>Product</td>\n",
       "      <td>Quantity Ordered</td>\n",
       "      <td>Price Each</td>\n",
       "      <td>Order Date</td>\n",
       "      <td>Purchase Address</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Order ID  Product  Quantity Ordered  Price Each  Order Date  \\\n",
       "121332  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "58411   Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "34529   Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "169827  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "147389  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "88576   Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "2463    Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "123475  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "166876  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "141764  Order ID  Product  Quantity Ordered  Price Each  Order Date   \n",
       "\n",
       "        Purchase Address  \n",
       "121332  Purchase Address  \n",
       "58411   Purchase Address  \n",
       "34529   Purchase Address  \n",
       "169827  Purchase Address  \n",
       "147389  Purchase Address  \n",
       "88576   Purchase Address  \n",
       "2463    Purchase Address  \n",
       "123475  Purchase Address  \n",
       "166876  Purchase Address  \n",
       "141764  Purchase Address  "
      ]
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Order ID\"].str.isdecimal() == False].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I wonder how that might have happened. Then again, this being a contrived dataset, I suppose the important thing is that we are careful to spot these things, rather than why; without knowing how the data was generated, it's almost certain that these rows were programmatically placed in the dataset.\n",
    "\n",
    "Regardless, let's get rid of them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>0</th>\n",
       "      <td>141234</td>\n",
       "      <td>iPhone</td>\n",
       "      <td>1</td>\n",
       "      <td>700</td>\n",
       "      <td>01/22/19 21:25</td>\n",
       "      <td>944 Walnut St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>1</th>\n",
       "      <td>141235</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>01/28/19 14:15</td>\n",
       "      <td>185 Maple St, Portland, OR 97035</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>2</th>\n",
       "      <td>141236</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>2</td>\n",
       "      <td>11.99</td>\n",
       "      <td>01/17/19 13:33</td>\n",
       "      <td>538 Adams St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>3</th>\n",
       "      <td>141237</td>\n",
       "      <td>27in FHD Monitor</td>\n",
       "      <td>1</td>\n",
       "      <td>149.99</td>\n",
       "      <td>01/05/19 20:33</td>\n",
       "      <td>738 10th St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>4</th>\n",
       "      <td>141238</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>01/25/19 11:59</td>\n",
       "      <td>387 10th St, Austin, TX 73301</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>...</th>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "      <td>...</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186845</th>\n",
       "      <td>319666</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>12/11/19 20:58</td>\n",
       "      <td>14 Madison St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186846</th>\n",
       "      <td>319667</td>\n",
       "      <td>AA Batteries (4-pack)</td>\n",
       "      <td>2</td>\n",
       "      <td>3.84</td>\n",
       "      <td>12/01/19 12:01</td>\n",
       "      <td>549 Willow St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186847</th>\n",
       "      <td>319668</td>\n",
       "      <td>Vareebadd Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>400</td>\n",
       "      <td>12/09/19 06:43</td>\n",
       "      <td>273 Wilson St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186848</th>\n",
       "      <td>319669</td>\n",
       "      <td>Wired Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>11.99</td>\n",
       "      <td>12/03/19 10:39</td>\n",
       "      <td>778 River St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>186849</th>\n",
       "      <td>319670</td>\n",
       "      <td>Bose SoundSport Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>99.99</td>\n",
       "      <td>12/21/19 21:45</td>\n",
       "      <td>747 Chestnut St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "<p>185950 rows × 6 columns</p>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                     Product Quantity Ordered Price Each  \\\n",
       "0        141234                      iPhone                1        700   \n",
       "1        141235    Lightning Charging Cable                1      14.95   \n",
       "2        141236            Wired Headphones                2      11.99   \n",
       "3        141237            27in FHD Monitor                1     149.99   \n",
       "4        141238            Wired Headphones                1      11.99   \n",
       "...         ...                         ...              ...        ...   \n",
       "186845   319666    Lightning Charging Cable                1      14.95   \n",
       "186846   319667       AA Batteries (4-pack)                2       3.84   \n",
       "186847   319668             Vareebadd Phone                1        400   \n",
       "186848   319669            Wired Headphones                1      11.99   \n",
       "186849   319670  Bose SoundSport Headphones                1      99.99   \n",
       "\n",
       "            Order Date                        Purchase Address  \n",
       "0       01/22/19 21:25         944 Walnut St, Boston, MA 02215  \n",
       "1       01/28/19 14:15        185 Maple St, Portland, OR 97035  \n",
       "2       01/17/19 13:33   538 Adams St, San Francisco, CA 94016  \n",
       "3       01/05/19 20:33      738 10th St, Los Angeles, CA 90001  \n",
       "4       01/25/19 11:59           387 10th St, Austin, TX 73301  \n",
       "...                ...                                     ...  \n",
       "186845  12/11/19 20:58  14 Madison St, San Francisco, CA 94016  \n",
       "186846  12/01/19 12:01    549 Willow St, Los Angeles, CA 90001  \n",
       "186847  12/09/19 06:43        273 Wilson St, Seattle, WA 98101  \n",
       "186848  12/03/19 10:39          778 River St, Dallas, TX 75001  \n",
       "186849  12/21/19 21:45  747 Chestnut St, Los Angeles, CA 90001  \n",
       "\n",
       "[185950 rows x 6 columns]"
      ]
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df = df[df[\"Order ID\"].str.isdecimal()]\n",
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's run the check again."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 12,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Order ID\"].str.isdecimal() == False].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "And now the same check with other columns:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Quantity Ordered\"].str.isdecimal() == False].count()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            0\n",
       "Product             0\n",
       "Quantity Ordered    0\n",
       "Price Each          0\n",
       "Order Date          0\n",
       "Purchase Address    0\n",
       "dtype: int64"
      ]
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Price Each\"].str.replace(\".\", \"\").str.isnumeric() == False].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks good so far. In order to verify that the `Order Date` column contains only date strings, we'll use the `parse` function from the `dateutil.parser` module."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from dateutil.parser import parse"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "We'll also write a simple function to handle parsing errors by returning `False`, and returning `True` for each successful parse:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "def is_date(s):\n",
    "    try:\n",
    "        parse(s, fuzzy=True)\n",
    "        return True\n",
    "    except:\n",
    "        return False"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Specifying `fuzzy=True` saves us the trouble of going through the data and ensuring all the valid date strings use the same format.\n",
    "\n",
    "Now we can check the `Order Date` column entries by calling `is_date()`, passing the entry string as the argument `s`:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/plain": [
       "Order ID            185950\n",
       "Product             185950\n",
       "Quantity Ordered    185950\n",
       "Price Each          185950\n",
       "Order Date          185950\n",
       "Purchase Address    185950\n",
       "dtype: int64"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df[df[\"Order Date\"].map(lambda s: not is_date(s))].count()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Looks like all values are accounted for and everything can be converted to its proper type! We'll do that before running some more data quality checks."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data type conversion"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df.loc[:, \"Order ID\"] = df[\"Order ID\"].astype(int)\n",
    "    df.loc[:, \"Quantity Ordered\"] = df[\"Quantity Ordered\"].astype(int)\n",
    "    df.loc[:, \"Price Each\"] = df[\"Price Each\"].astype(float)\n",
    "    df.loc[:, \"Order Date\"] = pd.to_datetime(df[\"Order Date\"], format=\"%m/%d/%y %H:%M\")\n",
    "except ValueError:\n",
    "    print(\"Couldn't convert a value.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185950 entries, 0 to 186849\n",
      "Data columns (total 6 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Order ID          185950 non-null  object\n",
      " 1   Product           185950 non-null  object\n",
      " 2   Quantity Ordered  185950 non-null  object\n",
      " 3   Price Each        185950 non-null  object\n",
      " 4   Order Date        185950 non-null  object\n",
      " 5   Purchase Address  185950 non-null  object\n",
      "dtypes: object(6)\n",
      "memory usage: 9.9+ MB\n"
     ]
    }
   ],
   "source": [
    "df.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>25944</th>\n",
       "      <td>165986</td>\n",
       "      <td>AAA Batteries (4-pack)</td>\n",
       "      <td>1</td>\n",
       "      <td>2.99</td>\n",
       "      <td>2019-03-13 19:41:00</td>\n",
       "      <td>103 Dogwood St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>165733</th>\n",
       "      <td>299470</td>\n",
       "      <td>Flatscreen TV</td>\n",
       "      <td>1</td>\n",
       "      <td>300.0</td>\n",
       "      <td>2019-12-21 17:14:00</td>\n",
       "      <td>996 Johnson St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>65746</th>\n",
       "      <td>203944</td>\n",
       "      <td>Macbook Pro Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>2019-05-05 11:07:00</td>\n",
       "      <td>246 Cherry St, Los Angeles, CA 90001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>7337</th>\n",
       "      <td>148218</td>\n",
       "      <td>Macbook Pro Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>1700.0</td>\n",
       "      <td>2019-01-21 16:02:00</td>\n",
       "      <td>868 5th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>164089</th>\n",
       "      <td>297903</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>2019-12-10 16:12:00</td>\n",
       "      <td>537 Park St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>81484</th>\n",
       "      <td>218959</td>\n",
       "      <td>Lightning Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>14.95</td>\n",
       "      <td>2019-06-29 21:32:00</td>\n",
       "      <td>16 7th St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>150068</th>\n",
       "      <td>284526</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>2019-11-08 12:36:00</td>\n",
       "      <td>814 Johnson St, Seattle, WA 98101</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>17803</th>\n",
       "      <td>158207</td>\n",
       "      <td>ThinkPad Laptop</td>\n",
       "      <td>1</td>\n",
       "      <td>999.99</td>\n",
       "      <td>2019-02-16 17:00:00</td>\n",
       "      <td>906 Maple St, Boston, MA 02215</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>21052</th>\n",
       "      <td>161336</td>\n",
       "      <td>Apple Airpods Headphones</td>\n",
       "      <td>1</td>\n",
       "      <td>150.0</td>\n",
       "      <td>2019-02-02 11:05:00</td>\n",
       "      <td>979 Cedar St, New York City, NY 10001</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>176233</th>\n",
       "      <td>309506</td>\n",
       "      <td>Google Phone</td>\n",
       "      <td>1</td>\n",
       "      <td>600.0</td>\n",
       "      <td>2019-12-08 06:22:00</td>\n",
       "      <td>395 14th St, Dallas, TX 75001</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "       Order ID                   Product Quantity Ordered Price Each  \\\n",
       "25944    165986    AAA Batteries (4-pack)                1       2.99   \n",
       "165733   299470             Flatscreen TV                1      300.0   \n",
       "65746    203944        Macbook Pro Laptop                1     1700.0   \n",
       "7337     148218        Macbook Pro Laptop                1     1700.0   \n",
       "164089   297903      USB-C Charging Cable                1      11.95   \n",
       "81484    218959  Lightning Charging Cable                1      14.95   \n",
       "150068   284526      USB-C Charging Cable                1      11.95   \n",
       "17803    158207           ThinkPad Laptop                1     999.99   \n",
       "21052    161336  Apple Airpods Headphones                1      150.0   \n",
       "176233   309506              Google Phone                1      600.0   \n",
       "\n",
       "                 Order Date                         Purchase Address  \n",
       "25944   2019-03-13 19:41:00         103 Dogwood St, Boston, MA 02215  \n",
       "165733  2019-12-21 17:14:00  996 Johnson St, New York City, NY 10001  \n",
       "65746   2019-05-05 11:07:00     246 Cherry St, Los Angeles, CA 90001  \n",
       "7337    2019-01-21 16:02:00      868 5th St, New York City, NY 10001  \n",
       "164089  2019-12-10 16:12:00           537 Park St, Seattle, WA 98101  \n",
       "81484   2019-06-29 21:32:00       16 7th St, New York City, NY 10001  \n",
       "150068  2019-11-08 12:36:00        814 Johnson St, Seattle, WA 98101  \n",
       "17803   2019-02-16 17:00:00           906 Maple St, Boston, MA 02215  \n",
       "21052   2019-02-02 11:05:00    979 Cedar St, New York City, NY 10001  \n",
       "176233  2019-12-08 06:22:00            395 14th St, Dallas, TX 75001  "
      ]
     },
     "execution_count": 20,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Marvelous."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data quality checks, continued"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now that we have data in the right format, let's make sure it's _good_ data, using a combination of data visualization and text output.\n",
    "\n",
    "Note: this will \"void the warranty,\" so to speak, of our data, in the sense that once we've performed these checks, we will have performed so many tests on the data that we can no longer run traditional hypothesis tests due to the [multiple comparisons problem](https://en.wikipedia.org/wiki/Multiple_comparisons_problem). The multiple comparisons problem will arise because every time we compare two variables or see the sample distribution of a variable, we will essentially have performed a hypothesis test on it. As more and more hypothesis tests are run on a set of data, the more likely it is that a spurious correlation, or a correlation that has arisen purely due to coincidence and doesn't exist in reality, will be encountered. There are ways (corrections) to account for that (see the \"Controlling procedures\" section of the linked Wikipedia page), but a) it's not the same as having \"fresh\" data, and b) we are not particularly interested in testing any hypotheses at the moment. In fact, analyzing this data is largely what will _lead_ to hypotheses we want to test! We just won't be able to test them, unless we can get another set of data from the same store, collected the same way as this data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "tags": []
   },
   "source": [
    "First, let's use the `pandas.DataFrame.describe()` method to get a quick overview of the data."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### General information"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>Order ID</th>\n",
       "      <th>Product</th>\n",
       "      <th>Quantity Ordered</th>\n",
       "      <th>Price Each</th>\n",
       "      <th>Order Date</th>\n",
       "      <th>Purchase Address</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>185950</td>\n",
       "      <td>185950</td>\n",
       "      <td>185950</td>\n",
       "      <td>185950.00</td>\n",
       "      <td>185950</td>\n",
       "      <td>185950</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>unique</th>\n",
       "      <td>178437</td>\n",
       "      <td>19</td>\n",
       "      <td>9</td>\n",
       "      <td>17.00</td>\n",
       "      <td>142395</td>\n",
       "      <td>140787</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>top</th>\n",
       "      <td>160873</td>\n",
       "      <td>USB-C Charging Cable</td>\n",
       "      <td>1</td>\n",
       "      <td>11.95</td>\n",
       "      <td>2019-12-15 20:16:00</td>\n",
       "      <td>193 Forest St, San Francisco, CA 94016</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>freq</th>\n",
       "      <td>5</td>\n",
       "      <td>21903</td>\n",
       "      <td>168552</td>\n",
       "      <td>21903.00</td>\n",
       "      <td>8</td>\n",
       "      <td>9</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "        Order ID               Product  Quantity Ordered  Price Each  \\\n",
       "count     185950                185950            185950   185950.00   \n",
       "unique    178437                    19                 9       17.00   \n",
       "top       160873  USB-C Charging Cable                 1       11.95   \n",
       "freq           5                 21903            168552    21903.00   \n",
       "\n",
       "                 Order Date                        Purchase Address  \n",
       "count                185950                                  185950  \n",
       "unique               142395                                  140787  \n",
       "top     2019-12-15 20:16:00  193 Forest St, San Francisco, CA 94016  \n",
       "freq                      8                                       9  "
      ]
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Here are a few observations we can make based on just this:\n",
    "1. The maximum `Quantity Ordered` is 9, yet the 75th percentile is 1; this means that more than one of an items is ordered less than 25% of the time. This is further supported by the standard deviation of `Quantity Ordered` being only ~0.44.\n",
    "2. The most expensive item in the store costs \\\\$1,700.\n",
    "3. There is an order on the first of January, 2020 &mdash; we should get rid of that data point (as well as any others that do not take place in 2019) to confine ourselves purely to 2019 data.\n",
    "4. Order dates are pretty evenly distributed: the minimum is January 1, the first quartile is mid April, the median is late-mid July, the third quartile is late October, and the maximum, is the first day of the new year."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now let's perform some simple visual analyses on each numeric column's distribution, and see what we can learn."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Data distributions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "Index: 185950 entries, 0 to 186849\n",
      "Data columns (total 3 columns):\n",
      " #   Column            Non-Null Count   Dtype \n",
      "---  ------            --------------   ----- \n",
      " 0   Quantity Ordered  185950 non-null  object\n",
      " 1   Price Each        185950 non-null  object\n",
      " 2   Order Date        185950 non-null  object\n",
      "dtypes: object(3)\n",
      "memory usage: 5.7+ MB\n"
     ]
    }
   ],
   "source": [
    "df[[\"Quantity Ordered\", \"Price Each\", \"Order Date\"]].hist()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Rough, but informative.\n",
    "\n",
    "The distribution of the values in the `Quantity Ordered` column confirms our earlier inference that most orders contain a single item. The histogram additionally tells us that hardly any orders contain more than three of any item.\n",
    "\n",
    "The `Order Date` distribution tells us how busy the store was during various times of the year. That end-of-year peak could mean that the store's sales are on the rise year-over-year, but I suspect it's more likely to be explained by the fact that December incentives shopping, due to Christmas, Boxing Day, and the New Year. We would need to see data for January and a bit later of 2020 to be certain.\n",
    "\n",
    "The `Price Each` histogram is particularly interesting. Inexpensive items comprise the vast majority of orders, which makes sense, but there is an out-of-place bump in sales of items costing more than \\\\$1,500. Why? Let's see what items have a price greater than \\\\$1,500 and see if we can infer the reason."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"Price Each\"] > 1500].sample(10)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "That makes sense, the Macbook Pro is an extremely popular personal computer.\n",
    "\n",
    "Occasionally, datasets will collect all values beyond a cutoff point into the same maximum value. For instance, a real estate dataset may have a price cutoff of \\\\$1,000,000, and any properies with values exceeding that will be counted as having a price of \\\\$1,000,000, resulting in a bump on the right edge of the price histogram. I'm glad to see that's not what happened here.\n",
    "\n",
    "Everything checks out in terms of data quality, so let's move on to data transformation!"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data transformation"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Remember that we got rid of several rows earlier, because they contained null values or contained the column names rather than any useful information; that messed up our index, because now there are \"holes\" in it. It's possibly not a big deal, but I prefer to have all the useful data logically sorted and indexed. So let's sort and reindex our DataFrame."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.sort_values(by = \"Order ID\", inplace = True)\n",
    "df.reset_index(drop = True, inplace = True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Next, remember that we discovered earlier that the maximum `Order Date` is January 1, 2020. Since this is an analysis of 2019 data, we don't want any 2020 orders, so let's get rid of those rows. But first, I'm curious to see how many 2020 orders made it into this data set."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "df.loc[df[\"Order Date\"] >= parse(\"01/01/2020\")]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Not many when compared to the size of the dataset, but a lot more than I expected. Instead of just discarding it, let's put this stuff into a new DataFrame, separate from our 2019 data. We'll also put the 2019 data into its own DataFrame. This ensures we have the original data, should we ever want to refer to it."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2020 = df.loc[df[\"Order Date\"] >= parse(\"01/01/2020\")].copy()\n",
    "data_2019 = df.loc[df[\"Order Date\"] < parse(\"01/01/2020\")].copy()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.describe()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2020.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Excellent. Now we can move on to analysis."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Data analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "A good rule of thumb in data analysis, especially in _exploratory_ data analysis, when we don't have a specific question we're interested in, is to get a big picture overview of general information and then get progressively more specific as questions arise.\n",
    "\n",
    "Since we're analyzing store data, let's start by checking out the year's sales statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Total sales in 2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Since the data records each type item sold at a time as its own order, the year's total sales will be equal to each order's `Quantity Ordered` multiplied by the item's `Price Each`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "pycharm": {
     "name": "#%%\n"
    },
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_sales = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).sum()\n",
    "total_sales"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So, in 2019, the store had a revenue of around \\\\$3,448,365.68. Not bad, though without knowing where the store is geographically, how many employees it has, and what its costs are, there's no way to tell whether that's objectively good. In a very high cost of living area, with many employees, and high costs, each employee and store owner may not have lavish lifestyles. On the other hand, if it's in a low cost of living area, with only a few employees, and not much cash outflow, the store's employees could be living like royalty.\n",
    "\n",
    "Let's get a bit more specific, and examine the monthly statistics."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {
    "pycharm": {
     "name": "#%% md\n"
    }
   },
   "source": [
    "### Monthly statistics\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The `Order Date` histogram we created previously in the data quality check section gave us an overview of how orders are distributed by month, but we don't know if sales (i.e. revenue) follow the same distribution. Let's find out!\n",
    "\n",
    "If we're going to be splitting data up based on unique values of a categorical variable, though, it's convenient to have a column containing specifically the values of that variable. Currently, the month of each order is tied up in its `Order Date`, and we'd have to parse each entry individually to extract the order's month. That's both computationally expensive and a messy thing to code. So let's create new colummns for the number and name of the month of each order (e.g. April is number 4), as well as separate columns for the day of the month, day of the year (e.g. February 1 is the 32nd day of the year), and weekday (Sunday, Monday, etc.). This will let us investigate such things as whether sales spike on December 24th, see sales as a per-day time series spanning the duration of 2019, and how the day of the week impacts sales.\n",
    "\n",
    "We will need to import the `datetime.datetime` module first."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "from datetime import datetime"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.loc[:, \"Month Number\"] = pd.Series(data_2019.loc[:, \"Order Date\"].map(lambda d: d.month)).astype(\"Int8\")\n",
    "data_2019.loc[:, \"Month Name\"] = pd.Series(data_2019.loc[:, \"Order Date\"].map(lambda d: d.month_name()))\n",
    "data_2019.loc[:, \"Day of Month\"] = pd.Series(data_2019.loc[:, \"Order Date\"].map(lambda d: d.day)).astype(\"Int8\")\n",
    "\n",
    "date_offset = data_2019.loc[:, \"Order Date\"].min().toordinal() + 1\n",
    "\n",
    "data_2019.loc[:, \"Ordinal Day\"] = pd.Series(data_2019.loc[:, \"Order Date\"].map(lambda d: d.toordinal() - date_offset)).astype(\"Int16\")\n",
    "data_2019.loc[:, \"Weekday\"] = pd.Series(data_2019.loc[:, \"Order Date\"].map(lambda d: d.day_name()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.sample(10)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.info()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.describe()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Perfect. Now, let's analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total monthly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_monthly_sales = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).groupby(data_2019[\"Month Number\"]).sum()\n",
    "total_monthly_sales.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "For comparison, here's the twelve-bin histogram of `Order Date`s:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019[\"Order Date\"].hist(bins=12)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The distribution of total monthly sales follows the approximate distribution of orders per month. No surprises there. This means that there aren't any major shifts in the percentage of orders per month that each item makes up.\n",
    "\n",
    "For example, suppose that in April, almost all orders were of expensive electronics like the Macbook Pro or the iPhone; in that case, the bar representing monthly sales in April would be significantly taller than it is, and the distribution would have a pretty glaring difference when compared to the orders-per-month distribution."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average monthly sales"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "average_monthly_sales = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).groupby(data_2019[\"Month Number\"]).mean()\n",
    "average_monthly_sales.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Hardly any variation in the average sales per month. Interesting. I would've thought that there would be a significant spike in August and September, thinking that a somewhat larger portion of sales would comprise new phones and laptops due to academic institutions starting a new year, and in December, due to the same change in portion but caused by Christmas and the new year.\n",
    "\n",
    "Maybe the monthly orders are different?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Total monthly orders"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "total_monthly_orders = data_2019.groupby(data_2019[\"Month Number\"])[\"Order ID\"].nunique()\n",
    "total_monthly_orders.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Nothing unexpected here either. Of course, if the distribution of the number of orders per month resembles the distribution of total sales per month, it's practically guaranteed that the distribution of the average price per order per month is going to be more or less a horizontal line, i.e. constant. Let's verify that, though."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Average spent per order per month"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "avg_monthly_order_price = total_monthly_sales / total_monthly_orders\n",
    "avg_monthly_order_price.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Approximately constant, as expected. In fact, let's be _really_ sure. Let's add a regression line to this bar plot.\n",
    "\n",
    "We will need `numpy` and `matplotlib`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By the way, in addition to the official documentation, I also recommend [this](https://realpython.com/python-matplotlib-guide/) excellent guide on the Real Python website, written by Brad Solomon, for a high-level understanding of Matplotlib."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "x = avg_monthly_order_price.index.to_list()\n",
    "y = avg_monthly_order_price.values\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "\n",
    "ax.bar(x, y, color=\"dodgerblue\")\n",
    "\n",
    "params = np.polyfit(x, y, 1)\n",
    "curve = np.poly1d(params)\n",
    "\n",
    "ax.plot(x, curve(x), \"black\")\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I also took the liberty of changing the bars' colour to a more attractive blue.\n",
    "\n",
    "Anyway, you can see that the regression line is almost perfectly horizontal, indicating almost no change in the monthly average spending per order.\n",
    "We can see the exact parameters of the line, too:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "params"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The line has a slope of around -0.4 and a y-intercept of around 196."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "I think that's the most we can really do with the monthly timeframe. The thing with months is that they're just long enough that the weather can change noticeably in between them, and only a few months exist between major social and economic events like the resuming of academics in September, Halloween in October, Christmas in December, Valentine's Day in February, spring break, summer vacation, etc. All of these things can affect purchases pretty significantly, so without being able to compare data for the same season over multiple years, monthly data turns out not to be terribly useful."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Daily statistics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Daily statistics are found in largely the same way as monthly statistics; I'm currently not too interested in those.\n",
    "\n",
    "I am, however, interested in..."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Demographics"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Where do most sales come from? Are people from certain areas more likely to purchase certain things? This is the interesting stuff!\n",
    "\n",
    "Let's start by analyzing the distribution of total sales by state. To do that, we'll create new columns for each order's customer's state, and also their city and zip code, just in case."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Further data transformation"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "data_2019.loc[:, \"State\"] = pd.Series(data_2019.loc[:, \"Purchase Address\"].map(lambda s: s.split(\",\")[2].strip()[0:2]))\n",
    "data_2019.loc[:, \"City\"] = pd.Series(data_2019.loc[:, \"Purchase Address\"].map(lambda s: s.split(\",\")[1].strip()))\n",
    "data_2019.loc[:, \"Zip Code\"] = pd.Series(data_2019.loc[:, \"Purchase Address\"].map(lambda s: s.split(\",\")[2].strip()[3:]))\n",
    "\n",
    "data_2019"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Lovely. Now we can analyze."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### What states do orders come from?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_orders = data_2019.groupby(\"State\")[\"Order ID\"].nunique()\n",
    "state_orders.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_orders.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like the states from which orders are made are, in descending order of number of orders, California, New York, Texas, Massachusetts, Georgia, Washington, Oregon, and Maine, with California ordering more than New York, Texas, and Massachusetts put together.\n",
    "\n",
    "But do more orders necessarily translate into more revenue from each state? Let's see."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Sales by state"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_sales = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).groupby(data_2019[\"State\"]).sum()\n",
    "state_sales.plot(kind=\"bar\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "state_sales.sort_values(ascending = False)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Looks like there is indeed a _very_ strong correlation between the number of orders and revenue from each state. Which makes sense, though, truth be told, I was hoping for something unexpected.\n",
    "\n",
    "Maybe there's something to see in the per-city data?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### Orders and revenue by city"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This time, let's skip the prose and get fancy with the code."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "city_orders = data_2019.groupby(\"City\")[\"Order ID\"].nunique()\n",
    "city_sales = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).groupby(data_2019[\"City\"]).sum().reindex(city_orders.index)\n",
    "\n",
    "x = city_orders.index.to_list()\n",
    "y1 = city_orders\n",
    "y2 = city_sales\n",
    "\n",
    "fig, (ax1, ax2) = plt.subplots(1, 2)\n",
    "\n",
    "ax1.bar(x, y1, color=\"dodgerblue\")\n",
    "ax2.bar(x, y2, color=\"dodgerblue\")\n",
    "\n",
    "ax1.set_title(\"Orders per city\")\n",
    "ax2.set_title(\"Revenue per city\")\n",
    "\n",
    "ax1.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax2.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Identical. The final topic to analyze is the products themselves."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Product analysis"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This section will be a sort of aggregate of exploration. We'll get the prices of all the products, examine how many orders each product has, and see how much each product contributed to yearly revenue in 2019."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_prices = data_2019[\"Price Each\"].groupby(data_2019[\"Product\"]).mean()\n",
    "product_orders = data_2019[\"Quantity Ordered\"].groupby(data_2019[\"Product\"]).sum().reindex(product_prices.index)\n",
    "product_sales_contributions = (data_2019[\"Quantity Ordered\"] * data_2019[\"Price Each\"]).groupby(data_2019[\"Product\"]).sum().reindex(product_prices.index) / total_sales  # We found total_sales near the beginning of this notebook\n",
    "\n",
    "y1 = product_prices\n",
    "y2 = product_orders\n",
    "y3 = product_sales_contributions\n",
    "x = product_prices.index.to_list()\n",
    "\n",
    "fig, (ax1, ax2, ax3) = plt.subplots(1, 3, figsize=(18, 8))\n",
    "\n",
    "ax1.bar(x, y1, color=\"dodgerblue\")\n",
    "ax2.bar(x, y2, color=\"dodgerblue\")\n",
    "ax3.bar(x, y3, color=\"dodgerblue\")\n",
    "\n",
    "ax1.set_title(\"Product prices\")\n",
    "ax2.set_title(\"Product orders\")\n",
    "ax3.set_title(\"Product revenue contributions\")\n",
    "\n",
    "ax1.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax2.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax3.tick_params(axis=\"x\", labelrotation=90)\n",
    "\n",
    "fig.tight_layout()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Unsurprisingly, the least expensive items sell the most, and vice versa.\n",
    "\n",
    "What is most interesting is the product sales contribution chart in the context of the other two. Here are some observations:\n",
    "1. The batteries, charging cables, and wired headphones, which are very inexpensive and sold the most, contributed hardly anything to the yearly revenue.\n",
    "2. On the other hand, the Macbook Pro, iPhone, and ThinkPad laptop, which are the three most expensive and least ordered products, contributed the most to yearly revenue.\n",
    "3. The appliances are mid-range in terms of price but sold so few units that their combined contribution to yearly revenue was less than 3%.\n",
    "4. The 20in monitor is cheap, sold a meager number of units, and had a yearly revenue contribution of less than 2%.\n",
    "5. The Vareebadd phone (yes, yes &mdash; remember this isn't real world data, most store owners are hesitant to have their yearly numbers be known so we make do with what we have) isn't terribly expensive, but sold very few unuits and had a yearly revenue contribution of around 2.5%.\n",
    "\n",
    "Based on this, we can make a few recommendations:\n",
    "1. Get rid of the appliances and focus exclusively on small electronics and their accessories.\n",
    "2. Get rid of the 20in monitor and Vareebadd phone and focus exclusively on brand name, high quality products.\n",
    "3. Things like batteries, charging cables, and wired headphones, though contributing very little to yearly revenue, are probably cheap enough to order in bulk and keep in stock, so maybe those things can be shown in a \"Recommended\" items section when the customer goes to checkout their order?\n",
    "\n",
    "Next, I want to plot the proportion of orders each product constitutes per month, as well as per state."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_order_proportions_per_month = data_2019.groupby([\"Month Number\", \"Product\"])[\"Quantity Ordered\"].sum()\n",
    "\n",
    "month_numbers = product_order_proportions_per_month.index.levels[0].to_list()\n",
    "x = product_order_proportions_per_month.index.levels[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "\n",
    "for month in month_numbers:\n",
    "    ax.scatter(x, product_order_proportions_per_month.loc[[month]] / total_monthly_orders[month])\n",
    "\n",
    "ax.set_title(\"Product revenue contributions per month\")\n",
    "ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax.legend(month_numbers)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Doesn't look like there's much variation between months, though it seems there was a slightly higher-than-normal demand for AAA batteries in May. Let's see the per-state plots."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "tags": []
   },
   "outputs": [],
   "source": [
    "product_order_proportions_per_state = data_2019.groupby([\"State\", \"Product\"])[\"Quantity Ordered\"].sum()\n",
    "\n",
    "states = product_order_proportions_per_state.index.levels[0].to_list()\n",
    "x = product_order_proportions_per_state.index.levels[1]\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "fig.tight_layout()\n",
    "\n",
    "for state in range(len(states)):\n",
    "    ax.scatter(x, product_order_proportions_per_state.loc[[states[state]]] / state_orders.reindex(product_order_proportions_per_state.index.levels[0])[state])\n",
    "\n",
    "ax.set_title(\"Product revenue contributions per state\")\n",
    "ax.tick_params(axis=\"x\", labelrotation=90)\n",
    "ax.legend(states)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Maine is at an extreme in most product revenue contributions. Why do they need so many USB-C cables and Apple Airpods, yet so few AAA batteries and lightning cables? What's going on in Maine?\n",
    "\n",
    "As a Canadian, I'm not familiar enough with US state demographics, and since this data isn't real, there's no way to tell if there actually is something funky in Maine or if this was done just for fun.\n",
    "\n",
    "Either way, I think that's a sufficient level of analysis. We got some useful information and questions worth looking into, as well as some recommendations we can make to the store owners to simplify their inventory and optimize revenue."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.12"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
